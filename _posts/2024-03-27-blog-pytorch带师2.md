---
title: 'Pytorchè¿›é˜¶ä¹‹æ‚ä¸ƒæ‚å…«çš„çŸ¥è¯†ç‚¹æ€»ç»“'
date: 2024-04-07
permalink: /posts/2024/03/blog-Pytorch/
star: superior
tags:
  - Pytorch
---

å¦‚æœä¸çŸ¥é“å¹²ä»€ä¹ˆï¼Œå°±å†™å†™åšå®¢æ€»ç»“æ€»ç»“

# `torch.nn.Parameter & torch.Tensor`

> å­¦è¿™ä¸ªä¸»è¦æ˜¯å› ä¸ºæˆ‘åœ¨æ·±åº¦å­¦ä¹ çš„ä¸Šæœºä¸­å¯¹äºâ€œå†»ç»“æƒé‡â€è¿™ä¸ªç›¸å¯¹å¸¸è§çš„æ“ä½œæ„Ÿåˆ°ä¸€è„¸æ‡µé€¼ï¼Œè¿™è¿˜æ˜¯åœ¨æé†’æˆ‘è‡ªå·±çš„torchå¹¶**ä¸ç†Ÿç»ƒ**ã€‚

å½“æ—¶çš„ä¸€ä¸ªæ“ä½œæ˜¯è¿™æ ·çš„ï¼Œåœ¨å‰50ä¸ªbatchä¸­å°†åŸå§‹ç½‘ç»œä¸­backboneä¸­çš„æƒé‡å»å†»ç»“ï¼Œåœ¨åé¢100ä¸ªbatchä¸­å°†æƒé‡å»è§£å†»ï¼Œè¿™æ ·çš„è¯å³åŠ å¿«äº†æ”¶æ•›ï¼Œä¹Ÿçœäº†å¾ˆå¤šäº‹æƒ…ã€‚

```python
# è¯·å°†classes_pathæŒ‡å‘æ•°æ®é›†å¯¹åº”çš„ç±»åˆ«
classes_path='model data/Helmet classes.txt'
#è¯·åœ¨æ­¤å†»ç»“ä¸»å¹²ç‰¹å¾æå–ç½‘ç»œéƒ¨åˆ†çš„å‚æ•°
for param in model.backbone.parameters():
    param.requires grad =False
#å½“å‰epochå·²ç»å¤§äºå†»ç»“å­¦ä¹ çš„ä»£æ•°ï¼Œè¯·åœ¨æ­¤å¯¹ä¸»å¹²ç‰¹å¾æå–ç½‘ç»œè§£å†»
for param in model.backbone.parameters():
    param.requires grad = True
```


##  `torch.nn.parameter`

`torch.nn.parameter`æ˜¯torchä¸­ä¸€ç§ç‰¹æ®Šç±»å‹çš„tensorï¼Œç”¨äºè¡¨ç¤ºç¥ç»ç½‘ç»œä¸­çš„å¯å­¦ä¹ å‚æ•°ï¼Œfor exampleï¼Œå…¨è¿æ¥å±‚ä¸­çš„`torch.nn.Linear()`ä¸­çš„å‚æ•°weightå’Œbiasã€‚
 
## ä¸`torch.tensor`çš„åŒºåˆ«

### æ˜¯å¦ä¼šè‡ªåŠ¨æ·»åŠ åˆ°æ¨¡å‹çš„å‚æ•°åˆ—è¡¨ä¸­

* ä½¿ç”¨`torch.nn.parameter`å®šä¹‰çš„å¼ é‡å¯ä»¥è¢«è‡ªåŠ¨çš„æ·»åŠ åˆ°æ¨¡å‹çš„å‚æ•°åˆ—è¡¨ä¸­ï¼Œå¹¶ä¸”å¯ä»¥é€šè¿‡`.parameters()`çš„æ–¹æ³•æˆ–è€…`.name_parameters()`çš„æ–¹æ³•åˆ—å‡º

```python
import torch
import torch.nn as nn

class Layer(nn.Module):
    def __init__(self):
        super().__init__()
        self.weight = torch.nn.Parameter(torch.tensor([1.,2.]))
    
    def forward(self,input):
        return input * self.weight
layer = Layer()
print(layer.weight.requires_grad)    
for name,param in layer.named_parameters():
    print(name)   # weight
    print(param)   
```
è¾“å‡ºç»“æœä¸ºï¼š
```bash
True
weight
Parameter containing:
tensor([1., 2.], requires_grad=True)
```

* è€Œæ™®é€šçš„`torch.tensor`å¯¹è±¡æ˜¯ä¸ä¼šè¢«è‡ªåŠ¨çš„æ·»åŠ åˆ°æ¨¡å‹çš„å‚æ•°åˆ—è¡¨ä¸­çš„ï¼Œå› æ­¤ä¹Ÿä¸ä¼šè¢«`.parameters()`çš„æ–¹æ³•æˆ–è€…`.name_parameters()`çš„æ–¹æ³•ç»™æŸ¥è¯¢åˆ°ã€‚

```python
import torch
import torch.nn as nn

class Layer(nn.Module):
    def __init__(self):
        super().__init__()
        self.weight = torch.tensor([1.,2.])
    
    def forward(self,input):
        return input * self.weight
layer = Layer()
print(layer.weight.requires_grad)    
for name,param in layer.named_parameters():
    print(name)   # weight
    print(param)   # æ— è¾“å‡ºï¼Œæ‰“å°ä¸å‡ºæ¥ä»»ä½•çš„ç»“æœï¼Œè¯´æ˜è¯¥å‚æ•°ç”šè‡³æœªè¢«æ”¾åˆ°å‚æ•°çš„åˆ—è¡¨ä¸­å»

```
output:
```bash
False
```

### required_gradå±æ€§

è¿™ä¸€ç‚¹ç›¸å¯¹å®¹æ˜“ç†è§£ï¼Œä¹Ÿå³ä½¿ç”¨`torch.nn.Parameter`å®šä¹‰çš„å¯¹è±¡å…¶`required_grad`å±æ€§é»˜è®¤ä¸ºTrue,è€Œä¸€ä¸ªç®€å•çš„`torch.Tensor`å®šä¹‰çš„å¼ é‡å…¶`required_grad`å±æ€§é»˜è®¤ä¸ºFalseã€‚

* `torch.nn.Parameter`å®šä¹‰çš„å¯¹è±¡çš„å±æ€§è¢«è®¾ç½®ä¸º`True`çš„æ—¶å€™ï¼Œä»–ä»¬ä¼šå‚ä¸è‡ªåŠ¨çš„æ±‚å¯¼ï¼Œå¹¶ä¸”å¯ä»¥è¢«ä¼˜åŒ–å™¨è‡ªåŠ¨åœ°æ›´æ–°ï¼Œè¿™ä¹Ÿè§£é‡Šäº†æˆ‘ä»¬å¹³æ—¶è®¾ç½®ä¼˜åŒ–å™¨æ—¶å€™ä¸ºä»€ä¹ˆéƒ½æ˜¯`optimizer = optim.SGD(mode.)`ã€‚

* å¯¹äºæ™®é€šçš„`torch.Tensor`å¯¹è±¡ï¼Œ**å³ä½¿å°†å…¶`required_grad`å±æ€§è®¾ç½®ä¸ºTrueï¼Œä¹Ÿä¸ä¼šè¢«è‡ªåŠ¨çš„æ·»åŠ åˆ°æ¨¡å‹çš„å‚æ•°ä¸­å»ï¼Œä¹Ÿä¸ä¼šè¢«ä¼˜åŒ–å™¨è‡ªåŠ¨æ›´æ–°**ã€‚


## `nn.Linear`

`nn.Linear`æ˜¯æˆ‘ä»¬ç‚¼ä¸¹æ—¶å€™ååˆ†å¸¸è§ä¸”ç®€æ˜“ä¸€ä¸ªæ“ä½œï¼Œä½†æ˜¯æœ‰äº†ä¸Šé¢è¿™äº›çŸ¥è¯†çš„å…ˆéªŒï¼Œæˆ‘ä»¬å°è±¡ä¸­çš„`nn.Linear`åº”è¯¥æœ‰æ‰€æ”¹å˜,ä¾‹å¦‚ï¼Œè‡³å°‘æˆ‘ä»¬è¦æ˜ç™½ï¼Œå…¶ä¸­çš„`weight`å’Œ`bias`å±æ€§éƒ½æ˜¯é€šè¿‡`torch.nn.Parameter`å°è£…èµ·æ¥çš„ã€‚

ç®€æ˜“ç‰ˆæœ¬çš„pytorchå®ç°

```python

    def __init__(self, in_features: int, out_features: int, bias: bool = True,device=None, dtype=None) -> None:
        factory_kwargs = {'device': device, 'dtype': dtype}
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = Parameter(torch.empty((out_features, in_features), **factory_kwargs))
        if bias:
            self.bias = Parameter(torch.empty(out_features, **factory_kwargs))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()

```

# `tensor.view & tensor.reshape`

å‘ç°ä¸€ç¯‡å†™çš„ååˆ†ğŸ‘çš„

# `torch.nn.Embedding`

> â€˜æˆ‘ä¸€ç›´å¯¹Embeddingæœ‰ä¸€å±‚æŠ½è±¡çš„ï¼Œæ¨¡ç³Šçš„è®¤è¯†â€™

å‚è€ƒ[æˆ‘æœ€çˆ±çš„bç«™upä¸»çš„å†…å®¹](https://www.bilibili.com/video/BV1wm4y187Cr/?spm_id_from=333.337.search-card.all.click&vd_source=32f9de072b771f1cd307ca15ecf84087)

## embeddingçš„åŸºç¡€æ¦‚å¿µ

`embedding`æ˜¯å°†è¯å‘é‡ä¸­çš„è¯æ˜ å°„ä¸ºå›ºå®šé•¿åº¦çš„è¯å‘é‡çš„æŠ€æœ¯ï¼Œå¯ä»¥å°†one_hotå‡ºæ¥çš„é«˜ç»´åº¦çš„ç¨€ç–çš„å‘é‡è½¬åŒ–æˆä½ç»´çš„è¿ç»­çš„å‘é‡

![ç›´è§‚æ˜¾ç¤ºè¯ä¸è¯ä¹‹é—´çš„å…³ç³»](image-1.png)



## é¦–å…ˆæ˜ç™½embeddingçš„è®¡ç®—è¿‡ç¨‹

- embedding module çš„å‰å‘è¿‡ç¨‹æ˜¯ä¸€ä¸ªç´¢å¼•(æŸ¥è¡¨)çš„è¿‡ç¨‹
    - è¡¨çš„å½¢å¼æ˜¯ä¸€ä¸ªmatrix ï¼ˆä¹Ÿå³ embedding.weight,learnabel parametersï¼‰
        - matrix.shape:(v,h)
            - v:vocabulary size
            - h:hidden dimension

    - å…·ä½“çš„ç´¢å¼•çš„è¿‡ç¨‹ï¼Œæ˜¯é€šè¿‡onehot+çŸ©é˜µä¹˜æ³•çš„å½¢å¼å®ç°çš„
    - input.shape:(b,s)
        - b: batch size
        - s: seq len 
    - embedding(input)=>(b,s,h)
    - **è¿™å…¶ä¸­å…³é”®çš„é—®é¢˜å°±æ˜¯(b,s)å’Œ(v,h)æ€ä¹ˆå˜æˆäº†(b,s,h)**
 
```python
>>> # an Embedding module containing 10 tensors of size 3
>>> embedding = nn.Embedding(10, 3)
>>> # a batch of 2 samples of 4 indices each
>>> input = torch.LongTensor([[1, 2, 4, 5], [4, 3, 2, 9]])
>>> embedding(input)
tensor([[[-0.0251, -1.6902,  0.7172],
         [-0.6431,  0.0748,  0.6969],
         [ 1.4970,  1.3448, -0.9685],
         [-0.3677, -2.7265, -0.1685]],

        [[ 1.4970,  1.3448, -0.9685],
         [ 0.4362, -0.4004,  0.9400],
         [-0.6431,  0.0748,  0.6969],
         [ 0.9124, -2.3616,  1.1151]]])
# >>> # example with padding_idx
# >>> embedding = nn.Embedding(10, 3, padding_idx=0)
# >>> input = torch.LongTensor([[0, 2, 0, 5]])
# >>> embedding(input)
# tensor([[[ 0.0000,  0.0000,  0.0000],
#          [ 0.1535, -2.0309,  0.9315],
#          [ 0.0000,  0.0000,  0.0000],
#          [-0.1655,  0.9897,  0.0635]]])
```

## One-Hot çŸ©é˜µä¹˜æ³•

ç›®å‰`one_hot`å¯ä»¥å¾ˆæ–¹ä¾¿åœ°åœ¨`torch.nn.functional`ä¸­è¿›è¡Œè°ƒç”¨ï¼Œå¯¹äºä¸€ä¸ª[batchsize,seqlength]çš„tensorï¼Œone_hotå‘é‡å¯ä»¥ååˆ†æ–¹ä¾¿çš„å°†å…¶è½¬åŒ–ä¸º[batchsize,seqlength,numclasses],æ­¤æ—¶ï¼Œå†ä¸[numclasses,h]è¿›è¡Œç›¸ä¹˜ï¼Œä»è€Œå¾—åˆ°æœ€ç»ˆçš„[b,s,v]

## å‚æ•°padding_idx

è¿™ä¸ªå‚æ•°çš„ä½œç”¨æ˜¯æŒ‡å®šæŸä¸ªä½ç½®çš„æ¢¯åº¦ä¸è¿›è¡Œæ›´æ–°ï¼Œä½†æ˜¯ä¸ºä»€ä¹ˆä¸è¿›è¡Œæ›´æ–°ï¼Œä»¥åŠåœ¨å“ªä¸ªä½ç½®ä¸è¿›è¡Œæ›´æ–°æˆ‘è¿˜æ²¡ææ˜ç™½....

```python
>>> # example with padding_idx
>>> embedding = nn.Embedding(10, 3, padding_idx=0)
>>> input = torch.LongTensor([[0, 2, 0, 5]])
>>> embedding(input)
tensor([[[ 0.0000,  0.0000,  0.0000],
         [ 0.1535, -2.0309,  0.9315],
         [ 0.0000,  0.0000,  0.0000],
         [-0.1655,  0.9897,  0.0635]]])

>>> # example of changing `pad` vector
>>> padding_idx = 0
>>> embedding = nn.Embedding(3, 3, padding_idx=padding_idx)
>>> embedding.weight
Parameter containing:
tensor([[ 0.0000,  0.0000,  0.0000],
        [-0.7895, -0.7089, -0.0364],
        [ 0.6778,  0.5803,  0.2678]], requires_grad=True)
>>> with torch.no_grad():
...     embedding.weight[padding_idx] = torch.ones(3)
>>> embedding.weight
Parameter containing:
tensor([[ 1.0000,  1.0000,  1.0000],
        [-0.7895, -0.7089, -0.0364],
        [ 0.6778,  0.5803,  0.2678]], requires_grad=True)
```

## å…³äºmax_norm

è¿™ä¸ªå‚æ•°ç”¨äºè®¾ç½®è¾“å‡ºå’Œæƒé‡å‚æ•°æ˜¯å¦ç»è¿‡äº†æ­£åˆ™åŒ–ã€‚



## å‚è€ƒèµ„æ–™
[Enzo_Ai](https://www.bilibili.com/video/BV1tC411L7Hh/?spm_id_from=333.788.0.0&vd_source=32f9de072b771f1cd307ca15ecf84087)

























