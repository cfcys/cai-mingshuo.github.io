---
title: 'Diffusion入门知识2'
date: 2023-12-07
permalink: /posts/2024/01/blog-diffusion2/
star: superior
tags:
  - 扩散模型
  - VAE
---

这篇博客介绍了我在入门diffusion时入门进一步学到的一些知识。

## 回顾
在入门知识1中，我们主要是了解到VAE的损失函数的简洁形式为

$$
loss=MSE(X,X^{\prime})+KL(N(\mu_{1}, \sigma_{1}^{2}), N(0,1)) 
$$

其中的$MSE(X,X^{\prime})$是就是解码器解码得到的向量和输入向量之间的差距，或者说是重构的损失（reconstruct loss），$KL(N(\mu_{1}, \sigma_{1}^{2}), N(0,1))$则是为了使得编码器生成的因变量更贴近于正态分布。

 
### 角度一---为什么要加入KL散度

* reconstruct loss计算的是解码器解码得到的向量和输入向量之间的MSE loss，这一项比较好理解，就是反映出vae生成的结果和输入之间的差异，对应的目标是使vae生成的结果和输入尽可能相似，具体的原理基本类似于最小二乘拟合的原理来衡量误差

* kl散度正则项相较于MES loss理解起来不是。比较正式的说法是使得编码器生成的隐变量尽可能符合标准正态分布。这种说法相当的拗口，常让人不明所以，比如为什么要使用这两个公示。 很直接的一种方法是让我们看看如果我们把reconstruct loss去掉，单单留下kl loss项会导致vae的编码空间变成什么样。这是我在网上找到的一张图片，中间显示的就是只采用kl散度训练后隐空间中的隐变量的分布。所有的输入向量均会被无差别地编码成标准正态分布。

![各种损失的效果](/images/blog/BlogDiffusion2/image.png)

> 直观上看来这个kl正则项的作用是使得编码器生成的隐变量符合标准正态分布。那么事实上如何呢？事实上也确实就是使得编码结果接近标准正态分布，只不过由于reconstruct loss的共同作用结果不会是标准正态分布，否则也就编码不出信息了。接下来就是为什么kl项能够使得编码结果符合标准正态分布呢，这就涉及到对kl loss项的公式分析了。


### 角度二---加入KL散度是为了符合正太分布，那为什么要符合正态分布呢？

> 我之前疑惑的是为什么要符合正态的分布呢，**这是我一直疑惑的点，为什么要引入这里的$\mu$和$\sigma^2$**这里我主要参考[这一篇](https://www.gwylab.com/note-vae.html)给出相应解释；

如下图所示，在编码的过程中，隐变量可以根据解码的数据编码出全圆月图和半圆月图，但是作为一个合格的**生成**模型,我们期望的是其可以生成3/4月亮或者1/4的月亮。

![全月图和半月图](/images/blog/BlogDiffusion2/image-1.png)#pic_center

解决这个问题的一个思路是引入噪声，是的原有的图片的编码区域编码一些，也即从一对一变到一对多的情况，使得图片的编码区域可以变大，如下图所示使得其中的空白编码点被掩盖掉：

![空白编码点被掩盖掉](/images/blog/BlogDiffusion2/image-2.png)#pic_center

这种情况下，如果给图片编码时候加入一点噪音，那么每张图片的编码点会出现在绿色箭头的范围内(噪音就是绿色范围对应的地方)，因此解码器在训练时候会把绿色范围内的点可以还原成这个绿色范围对应的原始图片。

而在两个绿色范围的交界处，也即中间地带，此处参杂了两种噪声，就很比较有可能产生我们想要的3/4月的形态。 但是这样还有一个问题，大家需要注意到图片中标注黄色的地方，是仍然没有被覆盖到的，也即仍然是个失真的点；

这种情况下的一个解决思路是把噪声无限拉长，“使得对于每一个样本，它的编码会覆盖整个编码空间，不过我们得保证，在原编码附近编码的概率最高，离原编码点越远，编码概率越低”，如下图： 这就是我们一直理解的**分布**的概念

![引入分布的概念](/images/blog/BlogDiffusion2/image-3.png)

总结 这种将编码器架构从离散转为连续的方法，就是变分编码器的精髓思想。

## VAE的理论基础

在GMM高斯混合模型的学习中，我们可以了解到任何一个存在的数据的分布，可以看作是若干个高斯分布的叠加，在下图中，若干个蓝色高斯分布叠加形成黑色的原始数据，可以被证明出(~~但具体证明是啥我现在也想不起来~~)：当叠加数量很多时(512个)，此时误差就变得很小很小了

![高斯分布的叠加](/images/blog/BlogDiffusion2/image-4.png)

那么根据这个理论，我们在编码时，就可以将一组数据(黑色的线$p(x)$)抽象成许多个高斯分布的值

![公式图1](/images/blog/BlogDiffusion2/image-5.png)

在标记黄色的公式中，$m$是对每个高斯分布的一个编号，每一个m对应一个小的高斯分布$N(\mu^{m},\Sigma^{m})$,此时$P(x)$就可以等价为所有这些高斯分布的叠加，这个公式是这样的：

$$P(x)=\sum_{m}P(m)P(x\vert m)$$

其中$p(m)$是每个编号出现的的概率，在每个m中，x都有一个它对应的均值和误差值。

此时我们可以看到这仍然是个离散的表达形式，个人感觉这样已经是有了一定的拟合效果了，但是为何不想之前讲的那样，把这个变成离散的形式，从而更加的准备，也不会有空白值的情况出现，连续的形式是这样的：

$$P(x)=\int\limits_{z}P(z)P(x\vert z)dz$$

其中有${z\sim N(0,1),x\vert z\sim N(\mu(z),\sigma(z))} （x\vert z指的是在z下的x分布）$,这个积分更加形象的样子可以参考下图，同时肯定这里为什么$z$一定要符合这个高斯的分布，很多地方都讲实际上并不一定要求这样

![连续积分的可视化效果](/images/blog/BlogDiffusion2/image-6.png)

#### 求解这个式子

> 但是首先，我好像并不知道为什么要求解这个式子，先看看再说

已知$p(z)$是个0，1的正态分布，而$p(x\vert z)$是未知的，但是我们知道$x\vert z\sim N(\mu(z),\sigma(z))$,因此我们真正需要求解的是，$\mu(z)$和$\sigma(z)$这两个函数的表述式;因为一些数学上的事情(~~具体俺也不懂~~),导致这$p(x)$的解很难得到，因此需要引入**两个神经网络**来帮助我们求解。

* Encoder的引入

第一个神经网络是Decoder，他求解的是$\mu$和$\sigma$这两个函数，这是我们求解$p(x\vert z)$的过程
![Alt text](/images/blog/BlogDiffusion2/image-7.png)

第二个神经网络叫做Encoder，它求解的结果是$q(z\vert x)$，$q$可以代表任何分布。

![Alt text](/images/blog/BlogDiffusion2/image-8.png)

> 你可能好奇我为什么先引出Decoder再引出Encoder，这样做的为了体现**Encoder主要是在辅助第一个Decoder求解$p(x\vert z)$**，这是整个VAE中最为巧妙的概念。

我们最开始求解的目标式子是：$P(x)=\int\limits_{z}P(z)P(x\vert z)dz$,$p(x)$越大，意味着数据点x在模型描述的分布中出现的概率越高，意味着模型可以更好地生成或者解释观测的数据；因此，最大化$p(x)$的过程也是最小化真实数据和模型输出之间的差异，这通常通过ELBO实现




## 参考资料

https://zhuanlan.zhihu.com/p/578619659