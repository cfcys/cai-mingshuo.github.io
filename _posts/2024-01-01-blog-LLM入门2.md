---
title: 'LLM入门2'
date: 2024-02-01
permalink: /posts/2024/01/blog-LLM入门2/
star: superior
tags:
  - LLM实践
  - 
---

最近跟着跑了很多LLM模型，但是很多都只是浅浅地跑一跑试试效果，感觉还是有必要把做了的东西写一下

## 关于训练一个Tokenizer

> [Q]：为什么要去训练一个Tokenizer？
> Tokenizer的训练是为了让其更好地适应特定的文本数据集和任务需求；在一些特定的任务中，默认的
> tokenizer可能对通用文本处理效果很好，但是当落实到特定的情况中时，会表现不是很好；因此
> tokenizer的训练可以使其学习到这些特定文本中独有的词汇，从而提高在特定任务中处理的准确性。

![训练Tokenizer的过程](image-1.png)

- 训练的过程不涉及权重或者反向传播
- tokenizer 的 processing pipeline
    - normalization  （转化成小写，并且去掉音调等）
    - pretokenization
    - tokenizer model
    - postprocesssing
- subword tokenization algorithms (subword: tokens are part of words) 会有可能把一个词分成多个子词
    - BPE: byte pair encoding
        - 迭代式地**添加**策略，不断地添加Token,直到达到我们指定的 target vocabulary size
    - word piece
    - unigram
        - 迭代式地**删除**策略，直到达到我们指定的 target vocabulary size

### 训练Tokenizer的代码


## BPE算法

其中的BPE算法最为经典和普适，，，，

## 资料引用
<https://zhuanlan.zhihu.com/p/622027410#:~:text=AGI%20%E6%98%AF%20Artificial%20General%20Intelligence%20%E7%9A%84%E7%BC%A9%E5%86%99%EF%BC%8C%E4%B8%AD%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%BA%E2%80%9C%E9%80%9A%E7%94%A8%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E2%80%9D%EF%BC%8C%E4%BA%A6%E8%A2%AB%E7%A7%B0%E4%B8%BA%20%E5%BC%BA%20AI%EF%BC%8C,%E8%AF%A5%E6%9C%AF%E8%AF%AD%E6%8C%87%E7%9A%84%E6%98%AF%E5%9C%A8%E4%BB%BB%E4%BD%95%E4%BD%A0%E5%8F%AF%E4%BB%A5%E6%83%B3%E8%B1%A1%E7%9A%84%E4%BA%BA%E7%B1%BB%E7%9A%84%E4%B8%93%20%E4%B8%9A%E9%A2%86%E5%9F%9F%E5%86%85%EF%BC%8C%E5%85%B7%E5%A4%87%20%E7%9B%B8%E5%BD%93%E4%BA%8E%E4%BA%BA%E7%B1%BB%E6%99%BA%E6%85%A7%E7%A8%8B%E5%BA%A6%E7%9A%84%20AI%20%EF%BC%8C%E4%B8%80%E4%B8%AA%20AGI%20%E5%8F%AF%E4%BB%A5%E6%89%A7%E8%A1%8C%E4%BB%BB%E4%BD%95%E4%BA%BA%E7%B1%BB%E5%8F%AF%E4%BB%A5%E5%AE%8C%20%E6%88%90%E7%9A%84%E6%99%BA%E5%8A%9B%E4%BB%BB%E5%8A%A1%E3%80%82>

