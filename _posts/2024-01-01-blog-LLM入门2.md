---
title: 'LLM入门2'
date: 2024-02-01
permalink: /posts/2024/01/blog-LLM入门2/
star: superior
tags:
  - LLM实践
  - 
---

最近跟着跑了很多LLM模型，但是很多都只是浅浅地跑一跑试试效果，感觉还是有必要把做了的东西写一下

## 关于训练一个Tokenizer

> [Q]：为什么要去训练一个Tokenizer？
> Tokenizer的训练是为了让其更好地适应特定的文本数据集和任务需求；在一些特定的任务中，默认的
> tokenizer可能对通用文本处理效果很好，但是当落实到特定的情况中时，会表现不是很好；因此
> tokenizer的训练可以使其学习到这些特定文本中独有的词汇，从而提高在特定任务中处理的准确性。

![训练Tokenizer的过程](image-1.png)

- 训练的过程不涉及权重或者反向传播
- tokenizer 的 processing pipeline
    - normalization  （转化成小写，并且去掉音调等）
    - pretokenization
    - tokenizer model
    - postprocesssing
- subword tokenization algorithms (subword: tokens are part of words) 会有可能把一个词分成多个子词
    - BPE: byte pair encoding
        - 迭代式地**添加**策略，不断地添加Token,直到达到我们指定的 target vocabulary size
    - word piece
    - unigram
        - 迭代式地**删除**策略，直到达到我们指定的 target vocabulary size

### 训练Tokenizer的代码


## BPE算法

其中的BPE算法最为经典和普适，，，，



## llama代码片段1

    class Llama:
        @staticmethod  
        def build(
            ckpt_dir: str,
            tokenizer_path: str,
            max_seq_len: int,
            max_batch_size: int,
            model_parallel_size: Optional[int] = None,
        ) -> "Llama":
            if not torch.distributed.is_initialized():
                torch.distributed.init_process_group("nccl")
            if not model_parallel_is_initialized():
                if model_parallel_size is None:
                    model_parallel_size = int(os.environ.get("WORLD_SIZE", 1))
                initialize_model_parallel(model_parallel_size)

            local_rank = int(os.environ.get("LOCAL_RANK", 0))
            torch.cuda.set_device(local_rank)

            # seed must be the same in all processes
            torch.manual_seed(1)

            if local_rank > 0:
                sys.stdout = open(os.devnull, "w")

            start_time = time.time()
            checkpoints = sorted(Path(ckpt_dir).glob("*.pth"))
            assert len(checkpoints) > 0, f"no checkpoint files found in {ckpt_dir}"
            assert model_parallel_size == len(
                checkpoints
            ), f"Loading a checkpoint for MP={len(checkpoints)} but world size is {model_parallel_size}"
            ckpt_path = checkpoints[get_model_parallel_rank()]
            checkpoint = torch.load(ckpt_path, map_location="cpu")
            with open(Path(ckpt_dir) / "params.json", "r") as f:
                params = json.loads(f.read())

            model_args: ModelArgs = ModelArgs(
                max_seq_len=max_seq_len,
                max_batch_size=max_batch_size,
                **params,
            )
            tokenizer = Tokenizer(model_path=tokenizer_path)
            model_args.vocab_size = tokenizer.n_words
            torch.set_default_tensor_type(torch.cuda.HalfTensor)
            model = Transformer(model_args)
            model.load_state_dict(checkpoint, strict=False)
            print(f"Loaded in {time.time() - start_time:.2f} seconds")

            return Llama(model, tokenizer)

*  `@staticmethod` 标记一个方法为静态的方法，也即这个方法可以在不创建实例的情况下进行调用;静态方法不用传入self的参数，但这也导致了其不可以调用这个类的一些属性。

*   ```python
    -> "Llama"
    ```
    这里的用于指示函数build的返回类型。具体来说，这意味着调用build函数将返回一个Llama类型的实例

*   ```python
    ` if not torch.distributed.is_initialized(): & torch.distributed.init_process_group("nccl") ` 
    ```
    设置是否初始化了分布式的环境，如果没有就去设置初始化。

*  ```python
    if not torch.distributed.is_initialized():
            torch.distributed.init_process_group("nccl")
        if not model_parallel_is_initialized():
            if model_parallel_size is None:
                model_parallel_size = int(os.environ.get("WORLD_SIZE", 1))
            initialize_model_parallel(model_parallel_size)
    ```
    这一段跟上面的是一样的，都是用于设置相关的并行化内容

## 资料引用
<https://zhuanlan.zhihu.com/p/622027410#:~:text=AGI%20%E6%98%AF%20Artificial%20General%20Intelligence%20%E7%9A%84%E7%BC%A9%E5%86%99%EF%BC%8C%E4%B8%AD%E6%96%87%E7%BF%BB%E8%AF%91%E4%B8%BA%E2%80%9C%E9%80%9A%E7%94%A8%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E2%80%9D%EF%BC%8C%E4%BA%A6%E8%A2%AB%E7%A7%B0%E4%B8%BA%20%E5%BC%BA%20AI%EF%BC%8C,%E8%AF%A5%E6%9C%AF%E8%AF%AD%E6%8C%87%E7%9A%84%E6%98%AF%E5%9C%A8%E4%BB%BB%E4%BD%95%E4%BD%A0%E5%8F%AF%E4%BB%A5%E6%83%B3%E8%B1%A1%E7%9A%84%E4%BA%BA%E7%B1%BB%E7%9A%84%E4%B8%93%20%E4%B8%9A%E9%A2%86%E5%9F%9F%E5%86%85%EF%BC%8C%E5%85%B7%E5%A4%87%20%E7%9B%B8%E5%BD%93%E4%BA%8E%E4%BA%BA%E7%B1%BB%E6%99%BA%E6%85%A7%E7%A8%8B%E5%BA%A6%E7%9A%84%20AI%20%EF%BC%8C%E4%B8%80%E4%B8%AA%20AGI%20%E5%8F%AF%E4%BB%A5%E6%89%A7%E8%A1%8C%E4%BB%BB%E4%BD%95%E4%BA%BA%E7%B1%BB%E5%8F%AF%E4%BB%A5%E5%AE%8C%20%E6%88%90%E7%9A%84%E6%99%BA%E5%8A%9B%E4%BB%BB%E5%8A%A1%E3%80%82>

