---
title: 'LLMå…¥é—¨2'
date: 2024-02-01
permalink: /posts/2024/01/blog-LLMå…¥é—¨2/
star: superior
tags:
  - LLMå®è·µ
  - 
---

è®­ç»ƒtokenizeræ›´åƒæ˜¯ä¸€ç§ç»å…¸æœºå™¨å­¦ä¹ çš„ç»Ÿè®¡æ¨¡å‹,æ˜¯ç¡®å®šæ€§çš„ï¼Œåœ¨ä»£ç ä¸­ä¹Ÿæ˜¯è¿™æ ·ï¼Œæ›´å¤šçš„åŠŸå¤«è¦ç”¨åœ¨æ•°æ®çš„å¤„ç†å’Œä¼˜åŒ–ä¸Šã€‚

# å…³äºè®­ç»ƒä¸€ä¸ªTokenizer

> [Q]ï¼šä¸ºä»€ä¹ˆè¦å»è®­ç»ƒä¸€ä¸ªTokenizerï¼Ÿ
> Tokenizerçš„è®­ç»ƒæ˜¯ä¸ºäº†è®©å…¶æ›´å¥½åœ°é€‚åº”ç‰¹å®šçš„æ–‡æœ¬æ•°æ®é›†å’Œä»»åŠ¡éœ€æ±‚ï¼›åœ¨ä¸€äº›ç‰¹å®šçš„ä»»åŠ¡ä¸­ï¼Œé»˜è®¤çš„
> tokenizerå¯èƒ½å¯¹é€šç”¨æ–‡æœ¬å¤„ç†æ•ˆæœå¾ˆå¥½ï¼Œä½†æ˜¯å½“è½å®åˆ°ç‰¹å®šçš„æƒ…å†µä¸­æ—¶ï¼Œä¼šè¡¨ç°ä¸æ˜¯å¾ˆå¥½ï¼›å› æ­¤
> tokenizerçš„è®­ç»ƒå¯ä»¥ä½¿å…¶å­¦ä¹ åˆ°è¿™äº›ç‰¹å®šæ–‡æœ¬ä¸­ç‹¬æœ‰çš„è¯æ±‡ï¼Œä»è€Œæé«˜åœ¨ç‰¹å®šä»»åŠ¡ä¸­å¤„ç†çš„å‡†ç¡®æ€§ã€‚

![è®­ç»ƒTokenizerçš„è¿‡ç¨‹](/images/blog/Blogllm/image-1.png)

## éœ€è¦æ˜ç¡®çš„å‡ ç‚¹åŸºç¡€çŸ¥è¯†

1. é¦–å…ˆè¦æ˜ç¡®ï¼Œç›®å‰æˆ‘ä»¬è®­ç»ƒtokenizeréƒ½æ˜¯åŸºäºä¸€ä¸ªæ—§çš„tokenizerå»è®­ç»ƒä¸€ä¸ªæ–°çš„tokenizerï¼Œ
2. è®­ç»ƒçš„è¿‡ç¨‹ä¸æ¶‰åŠæƒé‡æˆ–è€…åå‘ä¼ æ’­
- å¦‚ä¸Šå›¾ tokenizer çš„ processing pipeline ä¸º
    - normalization  ï¼ˆè¿™ä¸€æ­¥æ˜¯ä¸ºäº†å°†æ‰€æœ‰çš„txtæ–‡æœ¬è½¬åŒ–æˆå°å†™ï¼Œå¹¶ä¸”å»æ‰éŸ³è°ƒç­‰ï¼‰
    - pretokenization
    - tokenizer model
    - postprocesssing
- subword tokenization algorithms (subword: tokens are part of words) ä¼šæœ‰å¯èƒ½æŠŠä¸€ä¸ªè¯åˆ†æˆå¤šä¸ªå­è¯ï¼Œå¹¶æœ‰èƒ°è…ºç™Œè¿™å‡ ç§ç®—æ³•ï¼Œä½†æ˜¯åœ¨æ­¤å¤„
    - BPE: byte pair encoding
        - è¿­ä»£å¼åœ°**æ·»åŠ **ç­–ç•¥ï¼Œä¸æ–­åœ°æ·»åŠ Token,ç›´åˆ°è¾¾åˆ°æˆ‘ä»¬æŒ‡å®šçš„ target vocabulary size
    - word piece
    - unigram
        - è¿­ä»£å¼åœ°**åˆ é™¤**ç­–ç•¥ï¼Œç›´åˆ°è¾¾åˆ°æˆ‘ä»¬æŒ‡å®šçš„ target vocabulary size

### BPEç®—æ³• Byte-Pair Encodingç®—æ³•

å­—èŠ‚å¯¹ç¼–ç (BPE)ç®—æ³•æœ€åˆæ˜¯ä½œä¸ºå‹ç¼©æ–‡æœ¬çš„ç®—æ³•å¼€å‘çš„ï¼Œç„¶ååœ¨é¢„è®­ç»ƒGPTæ¨¡å‹æ—¶è¢«OpenAIç”¨äºæ ‡è®°åŒ–ã€‚è‡³ä»Šèµ·ä»ç„¶è¢«å¾ˆå¤šTransformeræ¶æ„çš„æ¨¡å‹ä½¿ç”¨ï¼ŒåŒ…æ‹¬GPT,GPT-2ç­‰ç­‰ã€‚

å‡è®¾æˆ‘ä»¬çš„è¯­æ–™åº“ä½¿ç”¨è¿™äº”ä¸ªè¯`["hug","pug","pun","hun","hugs"]`,è¿™äº”ä¸ªè¯çš„åŸºæœ¬è¯æ±‡è¡¨å°†æ˜¯`["b","g","h","n","p","s","u"]`,

è€Œpair(é…å¯¹)å°±æ˜¯é’ˆå¯¹åŸºæœ¬è¯æ±‡è¡¨ä¸­åŒæ—¶å‡ºç°çš„è¯æ±‡ï¼Œç»Ÿè®¡ä»–ä»¬åŒæ—¶å‡ºç°çš„æ¬¡æ•°å¤§å°ï¼Œå¦‚ä¸‹å›¾ä¸­ï¼Œå°†ugé…å¯¹ä¹‹åæ”¾åœ¨äº†ä¸€èµ·ã€‚
![alt text](image-4.png)
![è¿™æ˜¯ä¸€ä¸ªä¸æ–­å¾ªç¯çš„è¿­ä»£çš„è¿‡ç¨‹](image-5.png)


## è®­ç»ƒTokenizerçš„ç®€æ˜“ä»£ç 

> ä¸å¾®è°ƒå¤§æ¨¡å‹æ¯”è¾ƒç±»ä¼¼ï¼Œè®­ç»ƒtokenizerä¹Ÿæ˜¯ä¸€å¥—ç›¸å¯¹å›ºå®šçš„ä»£ç èŒƒå¼

* **å…ˆæ˜¯å¯¹è¿™æ ·ä¸€æ®µpythonä»£ç è¿›è¡Œç®€å•çš„tokenizeï¼Œç›´è§‚çš„çœ‹ä¸‹æ•ˆæœ**

```python
python_code = r'''def say_hello():
    print('Hello, World!')
    
# print hello
say_hello()
'''
print(python_code)
```

æ„å»ºä¸€ä¸ªé¢„è®­ç»ƒçš„gpt2çš„Tokenizeræ¨¡å‹

```python
tokenizer = AutoTokenizer.from_pretrained('gpt2') 
print(tokenizer(python_code)['input_ids'])    # æ‰“å°input_ids 
print(tokenizer(python_code).tokens())        # æ‰“å°tokens
```

ä»£ç è¾“å‡ºä¸º
```
[4299, 910, 62, 31373, 33529, 198, 220, 220, 220, 3601, 10786, 15496, 11, 2159, 0, 11537, 198, 220, 220, 220, 220, 198, 2, 3601, 23748, 198, 16706, 62, 31373, 3419, 198]
['def', 'Ä say', '_', 'hello', '():', 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä print', "('", 'Hello', ',', 'Ä World', '!', "')", 'ÄŠ', 'Ä ', 'Ä ', 'Ä ', 'Ä ', 'ÄŠ', '#', 'Ä print', 'Ä hello', 'ÄŠ', 'say', '_', 'hello', '()', 'ÄŠ']
```
å¯ä»¥çœ‹å‡ºï¼Œä¸Šé¢çš„input_idså¯¹åº”çš„æ˜¯Embeddingå±‚ä¸­å¯¹åº”çš„è¡Œå‘é‡çš„idï¼Œä¸‹é¢çš„å†…å®¹å°±æ˜¯æ‹†åˆ†å‡ºæ¥çš„tokenã€‚

> æ— è®ºå­¦ä»€ä¹ˆï¼Œéƒ½ä¸èƒ½ç®€å•çš„çœ‹è§†é¢‘å°±å®Œäº‹äº†

* **ä¸‹é¢æ˜¯è®­ç»ƒTokenizerçš„å…³é”®æ­¥éª¤**

å…ˆæ˜¯å»åŠ è½½ä¸€ä¸ªåˆé€‚çš„æ•°æ®é›†

```python
from datasets import load_dataset
dataset = load_dataset(xxxxx) 
```

```python
iter_dataset = iter(dataset)  # æŠŠæ•°æ®è½¬åŒ–ä¸ºè¿­ä»£çš„å½¢å¼
tokenizer = AutoTokenizer.from_pretrained('gpt2')  # æ„å»ºé¢„è®­ç»ƒçš„æ¨¡å‹
```

``` python 
new_tokenizer = tokenizer.train_new_from_iterator(batch_iterator(),  #è¿™é‡Œæ˜¯æˆ‘ä»¬çš„å¯¹æ•°æ®å†™çš„ä¸€ä¸ªæ‰¹æ¬¡åŒ–çš„è¿­ä»£å™¨
                                                  vocab_size=12500, 
                                                  initial_alphabet=base_vocab)
```

é’ˆå¯¹è¿™é‡Œçš„éå†ï¼Œæœ‰ä¸¤ç§æ¯”è¾ƒå¸¸ç”¨çš„æ–¹å¼ï¼š

```python
# æ–¹å¼ä¸€
def get_training_corpus():
    return (
        raw_datasets["train"][i : i + 1000]["whole_func_string"]
        for i in range(0, len(raw_datasets["train"]), 1000)
    )
training_corpus = get_training_corpus()

# æ–¹å¼äºŒï¼ˆyieldï¼‰
def get_training_corpus():
    dataset = raw_datasets["train"]
    for start_idx in range(0, len(dataset), 1000):
        samples = dataset[start_idx : start_idx + 1000]
        yield samples["whole_func_string"]
```
ä¸Šé¢è®­ç»ƒçš„è¿‡ç¨‹å¯èƒ½æ˜¯æ¯”è¾ƒè€—è´¹æ—¶é—´ï¼Œä½†æ˜¯è¿™æ ·ä¹‹åï¼Œä¸€ä¸ªç®€å•çš„æœ‰ç›‘ç£è®­ç»ƒçš„tokenizerå°±æ­å»ºå¥½äº†ğŸ˜



## èµ„æ–™å¼•ç”¨

[NLPä»0åˆ°1ä¹‹HuggingFaceå®æˆ˜](https://zhuanlan.zhihu.com/p/657047389)
[äº”é“å£çº³ä»€upä¸»çš„llmå®è·µç³»åˆ—æ•™ç¨‹](https://www.bilibili.com/video/BV1Sk4y1P7LK)
[è®²è§£BPEç®—æ³•çš„è§†é¢‘](https://www.bilibili.com/video/BV1Ko4y1A7jN/?spm_id_from=333.337.search-card.all.click&vd_source=32f9de072b771f1cd307ca15ecf84087)