---
title: 'LLM入门3之初识llama2'
date: 2024-03-01
permalink: /posts/2024/03/blog-LLM入门23/
star: superior
tags:
  - LLM实践
  - 
---

似乎LLM微调的终点是去做NLP算法工程师了？这样正好，反正cv也一点卷不动

# llama代码的解析

## llama代码片段1

    class Llama:
        @staticmethod  
        def build(
            ckpt_dir: str,
            tokenizer_path: str,
            max_seq_len: int,
            max_batch_size: int,
            model_parallel_size: Optional[int] = None,
        ) -> "Llama":
            if not torch.distributed.is_initialized():
                torch.distributed.init_process_group("nccl")
            if not model_parallel_is_initialized():
                if model_parallel_size is None:
                    model_parallel_size = int(os.environ.get("WORLD_SIZE", 1))
                initialize_model_parallel(model_parallel_size)

            local_rank = int(os.environ.get("LOCAL_RANK", 0))
            torch.cuda.set_device(local_rank)

            # seed must be the same in all processes
            torch.manual_seed(1)

            if local_rank > 0:
                sys.stdout = open(os.devnull, "w")

            start_time = time.time()
            checkpoints = sorted(Path(ckpt_dir).glob("*.pth"))
            assert len(checkpoints) > 0, f"no checkpoint files found in {ckpt_dir}"
            assert model_parallel_size == len(
                checkpoints
            ), f"Loading a checkpoint for MP={len(checkpoints)} but world size is {model_parallel_size}"
            ckpt_path = checkpoints[get_model_parallel_rank()]
            checkpoint = torch.load(ckpt_path, map_location="cpu")
            with open(Path(ckpt_dir) / "params.json", "r") as f:
                params = json.loads(f.read())

            model_args: ModelArgs = ModelArgs(
                max_seq_len=max_seq_len,
                max_batch_size=max_batch_size,
                **params,
            )
            tokenizer = Tokenizer(model_path=tokenizer_path)
            model_args.vocab_size = tokenizer.n_words
            torch.set_default_tensor_type(torch.cuda.HalfTensor)
            model = Transformer(model_args)
            model.load_state_dict(checkpoint, strict=False)
            print(f"Loaded in {time.time() - start_time:.2f} seconds")

            return Llama(model, tokenizer)

*  `@staticmethod` 标记一个方法为静态的方法，也即这个方法可以在不创建实例的情况下进行调用;静态方法不用传入self的参数，但这也导致了其不可以调用这个类的一些属性。

*   ```python
    -> "Llama"
    ```
    这里的用于指示函数build的返回类型。具体来说，这意味着调用build函数将返回一个Llama类型的实例

*   ```python
    ` if not torch.distributed.is_initialized(): & torch.distributed.init_process_group("nccl") ` 
    ```
    设置是否初始化了分布式的环境，如果没有就去设置初始化。

*  ```python
    if not torch.distributed.is_initialized():
            torch.distributed.init_process_group("nccl")
        if not model_parallel_is_initialized():
            if model_parallel_size is None:
                model_parallel_size = int(os.environ.get("WORLD_SIZE", 1))
            initialize_model_parallel(model_parallel_size)
    ```
    这一段跟上面的是一样的，都是用于设置相关的并行化内容



## 资料引用


